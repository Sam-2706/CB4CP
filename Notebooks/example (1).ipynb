{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "# Ensure PyTorch uses a single thread for better efficiency in small-scale tasks\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "# Load text files from a directory\n",
        "def load_text_files_from_directory(directory):\n",
        "    files_content = []\n",
        "    for file_name in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "        if os.path.isfile(file_path) and file_path.endswith('.txt'):\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                files_content.append(file.read().strip())\n",
        "    return files_content\n",
        "\n",
        "# Combine problem statements and editorials\n",
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def split_text_for_gpt2_rag(text, max_chunk_tokens=512):\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Predict token count of new chunk\n",
        "        temp_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "        token_count = len(tokenizer.encode(temp_chunk, add_special_tokens=False))\n",
        "\n",
        "        if token_count <= max_chunk_tokens:\n",
        "            current_chunk = temp_chunk\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence  # start new chunk\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def combine_problems_and_editorials(problem_statements_path, editorials_path, max_chunk_tokens=512):\n",
        "    problems = load_text_files_from_directory(problem_statements_path)\n",
        "    editorials = load_text_files_from_directory(editorials_path)\n",
        "\n",
        "    combined_chunks = []\n",
        "\n",
        "    for idx, (p, e) in enumerate(zip(problems, editorials)):\n",
        "        full_text = f\"Problem: {p}\\n\\nEditorial: {e}\"\n",
        "        chunks = split_text_for_gpt2_rag(full_text, max_chunk_tokens=max_chunk_tokens)\n",
        "\n",
        "        # Optional: tag each chunk with source ID\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            tagged_chunk = f\"[Doc {idx+1}, Chunk {i+1}]\\n{chunk}\"\n",
        "            combined_chunks.append(tagged_chunk)\n",
        "\n",
        "    return combined_chunks\n"
      ],
      "metadata": {
        "id": "R9Snkb8Fnxu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EDITORIALS_PATH = r\"/content/drive/MyDrive/Editorials\"\n",
        "PROBLEM_STATEMENTS_PATH = r\"/content/drive/MyDrive/Problem_statement\"\n",
        "\n",
        "# Combine problem statements and editorials\n",
        "documents = combine_problems_and_editorials(PROBLEM_STATEMENTS_PATH, EDITORIALS_PATH)"
      ],
      "metadata": {
        "id": "FCKL3VNcYhcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append(os.path.abspath('/content/drive/MyDrive/CB4CP'))"
      ],
      "metadata": {
        "id": "HQd5vN5HfHZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqXLDJIplWlu"
      },
      "outputs": [],
      "source": [
        "# from embeddings import CodeBERTEmbedder\n",
        "# from vectorstore import VectorStore\n",
        "# from retriever import RAGRetriever\n",
        "# from chatbot import CPChatbot\n",
        "\n",
        "system_message=\"You are a smart chatbot which solves competitive programming problems from codeforces. You have been trained on question sets from codeforces along with their editorial. You have to provide model solutions and help me to figure out the solutions of the problems. Do not hallucinate.\\n\"\n",
        "embedder=CodeBERTEmbedder()\n",
        "vectorstore=VectorStore()\n",
        "retriever=RAGRetriever(embedder=embedder,vector_store=vectorstore)\n",
        "chatbot=CPChatbot(retriever,system_message=system_message)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.add_batch(documents)\n",
        "# vectorstore.save()"
      ],
      "metadata": {
        "id": "5Jg4MQnhc-KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query=input(\"Enter query :\")\n",
        "chatbot.chat(user_query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "L9zedsm-dZDd",
        "outputId": "a7a4755c-b427-489c-bbca-4f72adfa1810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-45-1947322780.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter query :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f53e5488"
      },
      "source": [
        "After running the cell above to redefine the `VectorStore` class, please re-run the following cells to use the updated class:\n",
        "\n",
        "- Cell `KqXLDJIplWlu` (Initializes embedder, vectorstore, and retriever)\n",
        "- Cell `7VnU76-MgIbL` (Initializes chatbot)\n",
        "- Cell `5Jg4MQnhc-KS` (Adds documents to the vectorstore)\n",
        "- Cell `L9zedsm-dZDd` (Runs the chat query)\n",
        "\n",
        "Hopefully, this will resolve the `ValueError`. If you encounter a new error, please provide the full traceback."
      ]
    }
  ]
}